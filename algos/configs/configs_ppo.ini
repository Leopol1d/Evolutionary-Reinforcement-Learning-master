[MODEL_CONFIG]
; for the RMSprop optimizer
epsilon = 1e-5
alpha = 0.99
reward_gamma = 0.99
MAX_GRAD_NORM = 5
; roll out n steps 100
ROLL_OUT_N_STEPS = 200
; only remember the latest ROLL_OUT_N_STEPS 100
MEMORY_CAPACITY = 2500
; only use the latest ROLL_OUT_N_STEPS for training A2C 100
BATCH_SIZE = 200
ENTROPY_REG = 0.01
; seeds for pytorch, 0, 2000, 2021
torch_seed = 0
TARGET_UPDATE_STEPS = 5
TARGET_TAU = 0.01

; concurrent
training_strategy = concurrent
actor_hidden_size = 128
critic_hidden_size = 128
action_masking = True
; "regionalR", "global_R"
reward_type = global_R

[TRAIN_CONFIG]
MAX_EPISODES = 20000
EPISODES_BEFORE_TRAIN = 1500
EVAL_EPISODES = 3
EVAL_INTERVAL = 200
reward_scale = 20.
actor_lr = 5e-4
critic_lr = 5e-4
test_seeds = 0,25,50,75,100,125,150,175,200,325,350,375,400,425,450,475,500,525,550,575
train_step = 1000

[ENV_CONFIG]
; seed for the environment, 0, 2000, 2021
seed = 0
; [Hz]
simulation_frequency = 15
; time step
duration = 20
policy_frequency = 5
; policy_frequency = 10
COLLISION_REWARD = 200
HIGH_SPEED_REWARD = 1
HEADWAY_COST = 4
HEADWAY_TIME = 1.2
MERGING_LANE_COST = 4
; 1: easy,  2: medium,  3: hard
traffic_density = 2

[EA_CONFIG]
pop_size = 5
max_pop_size = 10
rollout_size = 2

weight_magnitude_limit = 10000000
elite_fraction = 0.2
crossover_prob = 0.15
mutation_prob = 0.9

